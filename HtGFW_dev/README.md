<html>
<h1>Hopping the Great Firewall</h1><br><br>
<h3><b>Disclaimer:</b> This project is being published under an entirely open source license. I'm going to skip the legal mumbo-jumbo and skip to the point: the purpose of this project is to be shared. That means the source code, the data, the findings, and the rest. I want it to be ripped apart, criticized, and analyzed. That is how I want this code and data to be used, regardless of consequence. Share it wide and use it how you wish. </h3><br><br>
<b>(From my Research Prospectus)</b><br>
With the advent of the internet, it was not long until those in power began to use the web for their own political. This subversion of the web took the form of propaganda, offensive security operations (hacking), and surveillance of internet users. Big data allowed for a boom in communication breakthroughs, not the least of which was surveillance and big data profiling of internet users. One of the frequently ignored issues brought about by the web revolution, though, is not what is being done or what is being said: it is what is being left out, what is being hidden. Censorship can take many forms, whether complete deletion of material online to the use of threats, coercion, or policy made to influence what users can and cannot say. That isn’t to say that all censorship is bad, as one must consider the censorship of pornography, for example, to be a good thing in the majority of cases. When the power to censor falls on a party with an agenda, though, censorship takes on a much more 1984 vibe. <br><br>
	One of the biggest purveyors of online censorship is the People’s Republic of China, the architects of the Great Firewall of China. The GFW is a plethora of technical and non-technical apparatus that are in place to monitor, surveil, and censor data on the mainland China, as well as to purvey propaganda that adheres to strict Communist Party guidelines. This apparatus uses human controls and technical surveillance systems to watch, listen, and censor material, whether “censor” takes the form of post deletion, IP-address banning, physical surveillance, or arrest. The CPC insists that this censorship of social media platforms is in place in order to ensure the safety and security of the homeland, with critics insisting it is used instead as a political tool to ensure a placated populous, especially in times of upheaval.<br><br>
	My project, therefore, is an attempt to scale the Great Firewall and allow a look over the edge. This technical approach is essentially two-parted: the first is to create a Python-based web-scraping tool aimed at popular Chinese social media site Zhihu (??). I will manually input potentially sensitive topics, such as politics, democracy, hacking, and history, and let the web-scraper gather posts in these topics. The second portion of the project is to analyze the data for signs of censorship, gathering data about posts that are censored in order to discern key words, topics, and personalities that are more subject to censorship. The analysis will take popularity, content, and political climate into account, and check for signs of censorship that include, but are not limited to, post and comment deletion, obvious changes in the post content, and potentially automated “botting” of the post, or flooding the post with negative comments by bots.<br><br>
	By the end of the research, my project should allow for a quantitative and qualitative data set on the true scale of the censorship of Chinese social media. This can allow us to understand what political topics are taboo during times of upheaval, what mechanisms are being used to censor the Chinese web, and inform the general public of the dangers of a vast surveillance and censorship regime like the Great Firewall. This quantitative and qualitative look will have inherent objectivity and could allow for valuable insight into Chinese culture, language, politics, and social media technology.<br><br>

(From the Research Design Specification)

<h2>Project Overview </h2><br><br>
The People’s Republic of China, the formal name of mainland China and, according to the leading party, all of its satellites, including Tibet, Hong Kong, and Taiwan, is not known for their internet freedom. Where America fiercely debates online over Net Neutrality, China has been beaten to submission by quite possibly the most advanced surveillance and censorship apparatus in the history of the net. This is a distinctly different problem, as Net Neutrality is pretty easily measured, both quantitatively, in slower connection speeds to certain sites or from certain ISPs, and qualitatively, in terms of megabytes per second and dollars per month. 
Censorship, though, is much different. It takes many di?erent forms, manifesting in “soft” censorship, such as using bots and patriotic net warriors to beat a user back into party line, or “hard” censorship in the direct deletion and manipulation of the questionable post or, in some cases, arrest. This is inherently di?cult to measure, at both a microscopic, site by site, post by post level, and macroscopic, in terms of the entire Chinese pseudo-intranet, level. 
That, at its core, is the goal of this project. By “scaling the Great Firewall”, I embark on a mission that is both larger and smaller than its name’s connotation. By measuring posts on potentially objectionable content on Chinese social media site Zhihu (???I want to put a number to the mysterious censorship mechanism employed by the Communist Party of China (CPC). This will be done by building an API for Zhihu from scratch, fetching posts by topic using the natural layout of the site, checking for posts that have been censored (changed or deleted), and running linguistics analysis on these posts to determine not just how many posts are being censored, but what are they discussing, when are they posted, and how may they be connected to current politics.

<h2>User Requirements</h2><br><br> 
I plan to present a user friendly GUI after ?nishing the CLI version of the program. This GUI will be web-based, to allow for potential scalability in the future, and to allow for a more accessible interface for the user. The GUI will feature analytics of censored material according to time, topic, and content in a way that allows a user to obtain some sort of actionable intelligence from the data. The goal of this project is not to just be a neat analytics tool. My goal is for the end-user to learn something about the data, about the online community in China, and about the issues they face.

<h2>Development Environment</h2><br><br> 
The development environment, for now, is a desktop computer with a GTX 1060 GPU, Intel i7 processor, and 2 TB of memory. The project, excluding the front end GUI, will be coded entirely in Python, as I prefer it for web scraping utilities. Analysis will likely be done with Python as well, at least in the beginning, on ?at text ?les, with the potential of upgrading to a database format if it assists in the e?ciency of the project.

<h2>Deployment Environment</h2><br><br> 
For the purpose of the assignment, the project will be deployed locally for the user. Because of the massive memory and RAM footprint this project will likely have, this is seen as a PoC environment only, to be eventually deployed to a stand-alone web server with a clustercomputing back end for analysis. This is the subject of my research, with the senior project portion focusing more on the miniaturized, local version instead of cluster computing back end.

<h2>Architecture</h2><br><br> 
Architecture is divided into a module to fetch the data, scraping it from the site, a module to monitor the fetched data for censorship,flagging censored material, and a module to analyze the ?agged data to gain insight into censored material.
The fetching module and monitoring module will run continuously, constantly fetching posts in real time and constantly monitoring it for censorship. The monitoring portion will consistently “kick” posts out of the monitoring queue past a certain period of time, as it isn’t likely that an older post will be censored, which will keep networking and memory footprint lower. 
The analysis portion will work in batches, either by a pre-determined size of a batch or by a pre-determined amount of time. The analysis will include analysis of the content of the posts, the time in which it was posted, the time in which it was censored, how it was changed if not deleted, and what words in the post were likely to have triggered the censorship mechanism. Comments, fetched with the posts and analyzed for censorship along with the post itself, will be analyzed for censorship as well as potentially in?ammatory language, a sign of possible “soft” censorship.
<h2>Implementation Strategies</h2><br><br> 
I decided to design the architecture the way I did for scalability. In the beginning, I plan to manually ?nd topics on Zhihu that are likely to be censored, like politics (???or hacking culture (???????, and use a relatively small amount of topics to monitor, in order to establish a PoC architecture. By design, this can be scaled to a larger size with more topics to monitor, leading to more posts and more potential for censorship. If moved to a cluster computing system with a web server, this project would have a signi?cant amount of networking and computing power, housing a much larger scope.

<h2>User Interfaces</h2><br><br> 
User interface is going to be simplistic, letting the data speak for itself. Depending on the observations made in the collections and analysis stages, the data presentation will be in the form of searchable graphs, based upon post time, content, and topic, and overall statistics representing the data set as a whole.

<h2>Test and Integration Plan</h2><br><br> 
Testing will be simple because of the modular design of the project. Testing will occur throughout the development of each module, and will revert back to earlier modules to ensure they work together. A primary concern will be the format of the fetched posts, as they will have to be compact enough to minimize footprint, but robust enough to provide data in the analysis stage. 

Testing of the fetching stage will primarily be centered around the development of the API. Research on US and Chinese internet yielded few results for a workable Zhihu API, so the development of this first stage will be from scratch. The fetching module will have to maintain robustness so as to deal with changes to the site and networking errors, as well as dealing with captchas and login forums as they arise.
The monitoring stage will be tough by design. This will depend on detecting censorship, which is known to happen but has not been quantified as of yet. Manually creating “censorship” by adding and deleting posts is a potential way to test the module, but in practice, much is left in the air in terms of testing.
The analysis stage will be based upon content (likely similar to a bag of words approach, to compare and contrast words in censored posts to determine which ones may affect potential censorship), time of posting and time of censorship (to determine possible crossover with common business hours in China, signifying the censorship may be manual instead of automated), and topic of posts (to determine “hotspots” that are more heavily censored than others). 
<h2>Minimal Viable Product (MVP)</h2><br><br>
Minimally, I would like to have a CLI version of the program built that does statistical analysis on the data gathered. I want to be able to fetch posts from different subjects, compare subjects in terms of how much censorship is occurring in the discussion of these subjects, and offer relatively deep statistical analysis of the censorship in different subjects. Using CLI to present this data will be rudimentary, but will at least present a working PoC of the project, while offering statistical insight into the subject matter.
</html>